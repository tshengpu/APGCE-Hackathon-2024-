{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c77719-5d5b-4545-8e26-2b8f3512d8e7",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66952f5-e226-465e-abcc-e1eb59f48e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/participant4/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Import available models, you can also explore other PyTorch models\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcracknet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cracknet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import loss\n",
    "from torch import optim\n",
    "from metrics import eval_metrics, get_epoch_acc\n",
    "from dataloader import DataLoader\n",
    "from cross_val import CrossVal\n",
    "from torchvision import transforms\n",
    "from eval import eval\n",
    "from config import ModelParameters\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import available models, you can also explore other PyTorch models\n",
    "from cracknet import cracknet\n",
    "from unet import UNet, UNetResnet\n",
    "from segnet import SegNet, SegResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7641c43b-aa23-4cd9-be9e-982df074a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757bf11-d774-4d60-a699-57a5e3bccdd6",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a19ce1-8fd3-4e9c-9432-04570d1b8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oneepoch(model, class_count, criterion, eval_metric, device, my_optimizer, my_lr_scheduler, dataloader):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    batch_acc_numerator = 0\n",
    "    batch_acc_denominator = 0\n",
    "    epoch_lr = my_lr_scheduler.get_last_lr()[0]\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        my_optimizer.zero_grad()\n",
    "        #print(inputs.size())\n",
    "        mask_pred = model(inputs)\n",
    "        loss = criterion(mask_pred, labels)\n",
    "        loss.backward()\n",
    "        my_optimizer.step()\n",
    "        \n",
    "        # batch_loss += loss\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch_acc_numerator_tmp, batch_acc_denominator_tmp = eval_metrics(mask_pred, labels, class_count, eval_metric)\n",
    "        batch_acc_numerator += batch_acc_numerator_tmp\n",
    "        batch_acc_denominator += batch_acc_denominator_tmp\n",
    "    my_lr_scheduler.step()\n",
    "    epoch_loss = batch_loss / len(dataloader)\n",
    "    epoch_acc = get_epoch_acc(batch_acc_numerator, batch_acc_denominator, eval_metric)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca771314-be0b-459d-a212-1a389063161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model, class_count, criterion, eval_metric, EPOCHS, DEVICE, my_optimizer, my_lr_scheduler=None, dataloaders=None, logging=False, model_name='model.pt'):\n",
    "    model.to(DEVICE)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    lr = []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss, epoch_train_acc, epoch_lr = train_oneepoch(model, class_count, criterion, eval_metric, DEVICE, my_optimizer, my_lr_scheduler, dataloaders[epoch%3]['train'])\n",
    "        epoch_val_loss, epoch_val_acc = eval(model, class_count, criterion, eval_metric, DEVICE, dataloaders[epoch%3]['val'])\n",
    "\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if logging:\n",
    "            # train_loss.append(epoch_train_loss.detach().cpu().numpy().tolist())\n",
    "            train_loss.append(epoch_train_loss)\n",
    "            train_acc.append(epoch_train_acc)\n",
    "            # val_loss.append(epoch_val_loss.detach().cpu().numpy().tolist())\n",
    "            val_loss.append(epoch_val_loss)\n",
    "            val_acc.append(epoch_val_acc)\n",
    "            lr.append(epoch_lr)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS - 1}: TrainLoss: {epoch_train_loss:.4f}, TrainAcc: {epoch_train_acc:.4f}, ValLoss: {epoch_val_loss:.4f}, ValAcc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    print('Best Acc: {:4f}'.format(best_val_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    torch.save(model, model_name + '.pt')\n",
    "    \n",
    "    # save training details\n",
    "    pd.DataFrame({'Epochs':range(EPOCHS), 'Learning Rate': lr, 'Training Loss': train_loss, \n",
    "                    'Training Acc': train_acc, 'Validation Loss': val_loss, \n",
    "                    'Validation Acc': val_acc}).to_csv(model_name + '.csv', index = False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d559991-d6e0-4d87-8b61-41dab8da2f17",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3465706",
   "metadata": {},
   "source": [
    "Inside the data directory, the structure should be following:\n",
    "- train\n",
    "    - images\n",
    "        - IL 991.png\n",
    "        - IL 992.png\n",
    "    - labels\n",
    "        - IL 991.npy\n",
    "        - IL 992.npy\n",
    "    - class_names.txt\n",
    "- val\n",
    "    - images\n",
    "        - IL 993.png\n",
    "    - labels\n",
    "        - IL 993.npy\n",
    "    - class_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ab64e",
   "metadata": {},
   "source": [
    "class_names.txt is to specify the label class name for the training\n",
    "\n",
    "Example content of class_names.txt:\n",
    "\n",
    "\\_background_ <br>\n",
    "fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc9b1a1-ae9e-4422-8724-80f4c7d8a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the data directory and model filename\n",
    "DIR = 'data/' # Data directory\n",
    "MODEL_FILENAME = 'cracknet.pt' # Model filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb8c8e-48da-494f-bc92-d490b109bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fault = []\n",
    "num_horizon = []\n",
    "num_seismic = []\n",
    "\n",
    "for name in os.listdir('../data/aug_fault_mask'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    code = name.replace(\"fault\",\"\").replace(\".npy\",\"\")\n",
    "    if os.path.isfile('../data/aug_horizon_mask/horizon{}.npy'.format(code)) and os.path.isfile('../data/aug_raw_seismic/seismic{}.png'.format(code)):\n",
    "        num_fault.append(name)\n",
    "        num_horizon.append('horizon{}.npy'.format(code))\n",
    "        num_seismic.append('seismic{}.png'.format(code))\n",
    "    \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'RAW_SEISMIC': [f\"{x}\" for x in num_seismic],\n",
    "    'RAW_FAULT': [f\"{x}\" for x in num_fault],\n",
    "    'RAW_HORIZON': [f\"{x}\" for x in num_horizon]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9010f4-bc81-4cf7-b8d9-26200bd05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossVal(df, 3)\n",
    "dataloaders = cv\n",
    "# dataset = {}\n",
    "# dataset['train'] = LabelMe(data_folder=os.path.join(DIR,'train'), transform=data_transforms['train'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataset['val'] = LabelMe(data_folder=os.path.join(DIR,'val'), transform=data_transforms['val'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size = BATCH_SIZE,\n",
    "#                                             shuffle = True, num_workers = 8, \n",
    "#                                             drop_last = False)\n",
    "#                                             for x in ['train', 'val']}\n",
    "class_count = len(dataloaders[0]['train'].dataset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a2a6ec-bb90-470a-a0dc-c3c18015ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model for training, you can refer to the models that have been imported above\n",
    "model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "\n",
    "my_optimizer = optim.Adam(model.parameters(), lr = ModelParameters.LEARNING_RATE) # Check https://pytorch.org/docs/stable/optim.html for other optimizers\n",
    "my_lr_scheduler = optim.lr_scheduler.StepLR(my_optimizer, step_size=25, gamma=0.1) # Check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate for other schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ebbe8-74ed-4e6d-8e20-9aedbb05e3ff",
   "metadata": {},
   "source": [
    "## Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef98cd8-aef9-458f-9f02-e74fa7836f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([6, 3, 512, 512])\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "train_main(model, class_count, ModelParameters.CRITERION, ModelParameters.EVAL_METRIC, ModelParameters.EPOCHS, DEVICE, my_optimizer, my_lr_scheduler, dataloaders, logging = ModelParameters.LOGGING, model_name = MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbbdb2f-4b7f-418c-9213-372eb688f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "dl = DataLoader([],[],[], (512, 512))\n",
    "img = []\n",
    "img.append(dl.input_transforms(Image.open(os.path.join(dl.RAW_SEISMIC_FOLDER, df.at[idx, \"RAW_SEISMIC\"]))))\n",
    "img.append(dl.input_transforms(Image.open(os.path.join(dl.RAW_SEISMIC_FOLDER, df.at[idx+1, \"RAW_SEISMIC\"]))))\n",
    "img = torch.stack(img, dim=0)\n",
    "# fault_name = '.'.join(self.img_name[idx].split('.', maxsplit = 1)[:-1]) + '.npy'\n",
    "# fault = np.load(os.path.join(self.RAW_FAULT_FOLDER, self.fault_files[idx]), allow_pickle=True).astype(np.uint8)\n",
    "# fault = cv2.resize(mask, self.img_size, cv2.INTER_AREA)\n",
    "# fault_img = Image.open(os.path.join(self.RAW_FAULT_FOLDER, self.fault_files[idx]))\n",
    "# fault = self.output_transforms(fault_img)\n",
    "# fault  = np.array(fault_img)\n",
    "# mask_name = '.'.join(self.img_name[idx].split('.', maxsplit = 1)[:-1]) + '.npy'\n",
    "mask = []\n",
    "mask.append(torch.from_numpy(cv2.resize(np.load(os.path.join(dl.FAULT_MASK_FOLDER, df.at[idx, \"RAW_FAULT\"])).astype(np.uint8), dl.IMAGE_SIZE, cv2.INTER_AREA).astype(np.int64)))\n",
    "mask.append(torch.from_numpy(cv2.resize(np.load(os.path.join(dl.FAULT_MASK_FOLDER, df.at[idx+1, \"RAW_FAULT\"])).astype(np.uint8), dl.IMAGE_SIZE, cv2.INTER_AREA).astype(np.int64)))\n",
    "mask = torch.stack(mask, dim=0)\n",
    "img = img.to(DEVICE)\n",
    "mask = mask.to(DEVICE)\n",
    "mask_pred = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa8337e-bd28-4099-b190-6c7b43b12231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2678, device='cuda:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelParameters.CRITERION(mask_pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "397b5a78-84ce-4c1f-a522-4dad90334a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(524288), array(524288))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics(mask_pred, mask, class_count,'batch_pix_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "181ad91c-9296-4878-a9f9-8c31b661447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predict = torch.max(mask_pred.data, 1)\n",
    "# predict = predict + 1\n",
    "# mask = mask + 1\n",
    "# labeled = (mask > 0) * (mask <= class_count)\n",
    "# pixel_labeled = labeled.sum()\n",
    "# pixel_correct = ((predict == mask) * labeled).sum()\n",
    "# (pixel_correct.cpu().numpy(), pixel_labeled.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5123963c-65c8-4bb2-b3fa-6392fedc0248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.7269, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 3.7026],\n",
       "         [3.7230, 3.7357, 3.7314,  ..., 3.7001, 3.7036, 0.0000],\n",
       "         [3.7155, 3.7183, 3.7125,  ..., 0.0000, 0.0000, 3.7095],\n",
       "         ...,\n",
       "         [3.7073, 3.7086, 3.7009,  ..., 0.0000, 3.7119, 0.0000],\n",
       "         [0.0000, 3.7048, 0.0000,  ..., 3.7084, 3.7062, 3.6991],\n",
       "         [0.0000, 3.6966, 3.7024,  ..., 3.7000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[3.7173, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 3.7102],\n",
       "         [3.7153, 3.7252, 3.7193,  ..., 3.7127, 3.7187, 0.0000],\n",
       "         [3.7151, 3.7186, 3.7075,  ..., 0.0000, 0.0000, 3.7166],\n",
       "         ...,\n",
       "         [3.7114, 3.7085, 3.7084,  ..., 0.0000, 3.7113, 0.0000],\n",
       "         [0.0000, 3.7042, 0.0000,  ..., 3.7132, 3.7225, 3.6966],\n",
       "         [0.0000, 3.7061, 3.7064,  ..., 3.7055, 0.0000, 0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pred.data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cf6a5492-43a0-4926-b693-8cb324f5275a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f22c9-31cf-45c4-b5e1-df21bda7012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cracknet_model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "cracknet_model_total_params = sum(p.numel() for p in cracknet_model.parameters())\n",
    "cracknet_model_total_params\n",
    "\n",
    "unet_model = UNet(num_classes = class_count)\n",
    "unet_model_total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "unet_resnet_model = UNetResnet(num_classes = class_count)\n",
    "unet_resnet_model_total_params = sum(p.numel() for p in unet_resnet_model.parameters())\n",
    "\n",
    "segnet_model = SegNet(num_classes = class_count)\n",
    "segnet_model_total_params  = sum(p.numel() for p in segnet_model.parameters())\n",
    "segresnet_model = SegResNet(num_classes = class_count)\n",
    "segresnet_model_total_params  = sum(p.numel() for p in segresnet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af72315d-9840-40cd-a2d6-9bef818b53cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49320390"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cracknet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe041f0-45c4-4322-a26d-041e2d5a5ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26355234"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f046dc-628b-4bcc-90ea-e27fe58c09b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29999920"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_resnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b4db4e-1081-4135-a13b-d3ed44037a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16310850"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58aaba51-8226-478c-95f9-001dad495d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53553346"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segresnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163c4feb-3818-4d87-b54a-200ef2172c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.023778037318717"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cracknet_model_total_params/segnet_model_total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
