{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c77719-5d5b-4545-8e26-2b8f3512d8e7",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66952f5-e226-465e-abcc-e1eb59f48e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import loss\n",
    "from torch import optim\n",
    "from metrics import eval_metrics, get_epoch_acc\n",
    "from dataloader import DataLoader\n",
    "from cross_val import CrossVal\n",
    "from torchvision import transforms\n",
    "from eval import eval\n",
    "from config import ModelParameters\n",
    "\n",
    "# Import available models, you can also explore other PyTorch models\n",
    "from cracknet import cracknet\n",
    "from unet import UNet, UNetResnet\n",
    "from segnet import SegNet, SegResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7641c43b-aa23-4cd9-be9e-982df074a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757bf11-d774-4d60-a699-57a5e3bccdd6",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a19ce1-8fd3-4e9c-9432-04570d1b8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oneepoch(model, class_count, criterion, eval_metric, device, my_optimizer, my_lr_scheduler, dataloader):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    batch_acc_numerator = 0\n",
    "    batch_acc_denominator = 0\n",
    "    epoch_lr = my_lr_scheduler.get_last_lr()[0]\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        my_optimizer.zero_grad()\n",
    "        mask_pred = model(inputs)\n",
    "        loss = criterion(mask_pred, labels)\n",
    "        loss.backward()\n",
    "        my_optimizer.step()\n",
    "        \n",
    "        # batch_loss += loss\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch_acc_numerator_tmp, batch_acc_denominator_tmp = eval_metrics(mask_pred, labels, class_count, eval_metric)\n",
    "        batch_acc_numerator += batch_acc_numerator_tmp\n",
    "        batch_acc_denominator += batch_acc_denominator_tmp\n",
    "    my_lr_scheduler.step()\n",
    "    epoch_loss = batch_loss / len(dataloader)\n",
    "    epoch_acc = get_epoch_acc(batch_acc_numerator, batch_acc_denominator, eval_metric)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca771314-be0b-459d-a212-1a389063161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model, class_count, criterion, eval_metric, EPOCHS, DEVICE, my_optimizer, my_lr_scheduler=None, dataloaders=None, logging=False, model_name='model.pt'):\n",
    "    model.to(DEVICE)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    lr = []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss, epoch_train_acc, epoch_lr = train_oneepoch(model, class_count, criterion, eval_metric, DEVICE, my_optimizer, my_lr_scheduler, dataloaders['train'])\n",
    "        epoch_val_loss, epoch_val_acc = eval(model, class_count, criterion, eval_metric, DEVICE, dataloaders['val'])\n",
    "\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if logging:\n",
    "            # train_loss.append(epoch_train_loss.detach().cpu().numpy().tolist())\n",
    "            train_loss.append(epoch_train_loss)\n",
    "            train_acc.append(epoch_train_acc)\n",
    "            # val_loss.append(epoch_val_loss.detach().cpu().numpy().tolist())\n",
    "            val_loss.append(epoch_val_loss)\n",
    "            val_acc.append(epoch_val_acc)\n",
    "            lr.append(epoch_lr)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS - 1}: TrainLoss: {epoch_train_loss:.4f}, TrainAcc: {epoch_train_acc:.4f}, ValLoss: {epoch_val_loss:.4f}, ValAcc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    print('Best Acc: {:4f}'.format(best_val_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    torch.save(model, model_name + '.pt')\n",
    "    \n",
    "    # save training details\n",
    "    pd.DataFrame({'Epochs':range(EPOCHS), 'Learning Rate': lr, 'Training Loss': train_loss, \n",
    "                    'Training Acc': train_acc, 'Validation Loss': val_loss, \n",
    "                    'Validation Acc': val_acc}).to_csv(model_name + '.csv', index = False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d559991-d6e0-4d87-8b61-41dab8da2f17",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3465706",
   "metadata": {},
   "source": [
    "Inside the data directory, the structure should be following:\n",
    "- train\n",
    "    - images\n",
    "        - IL 991.png\n",
    "        - IL 992.png\n",
    "    - labels\n",
    "        - IL 991.npy\n",
    "        - IL 992.npy\n",
    "    - class_names.txt\n",
    "- val\n",
    "    - images\n",
    "        - IL 993.png\n",
    "    - labels\n",
    "        - IL 993.npy\n",
    "    - class_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ab64e",
   "metadata": {},
   "source": [
    "class_names.txt is to specify the label class name for the training\n",
    "\n",
    "Example content of class_names.txt:\n",
    "\n",
    "\\_background_ <br>\n",
    "fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc9b1a1-ae9e-4422-8724-80f4c7d8a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the data directory and model filename\n",
    "DIR = 'data/' # Data directory\n",
    "MODEL_FILENAME = 'cracknet.pt' # Model filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfb8c8e-48da-494f-bc92-d490b109bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fault = []\n",
    "num_horizon = []\n",
    "num_seismic = []\n",
    "indeces = [v for v in range(1000, 1441)]\n",
    "for name in os.listdir('../data/raw_fault'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "\n",
    "    num_fault.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "for name in os.listdir('../data/raw_horizon'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    num_horizon.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "for name in os.listdir('../data/raw_seismic'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    num_seismic.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'RAW_SEISMIC': [f\"seismic-{x}.png\" if x in num_seismic else None for x in indeces],\n",
    "    'RAW_FAULT': [f\"fault-{x}.npy\" if x in num_fault else None for x in indeces],\n",
    "    'RAW_HORIZON': [f\"horizon-{x}.npy\" if x in num_horizon else None for x in indeces]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9010f4-bc81-4cf7-b8d9-26200bd05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossVal(df, 3)\n",
    "dataloaders = cv[0]\n",
    "# dataset = {}\n",
    "# dataset['train'] = LabelMe(data_folder=os.path.join(DIR,'train'), transform=data_transforms['train'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataset['val'] = LabelMe(data_folder=os.path.join(DIR,'val'), transform=data_transforms['val'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size = BATCH_SIZE,\n",
    "#                                             shuffle = True, num_workers = 8, \n",
    "#                                             drop_last = False)\n",
    "#                                             for x in ['train', 'val']}\n",
    "class_count = len(dataloaders['train'].dataset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a2a6ec-bb90-470a-a0dc-c3c18015ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model for training, you can refer to the models that have been imported above\n",
    "model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "\n",
    "my_optimizer = optim.Adam(model.parameters(), lr = ModelParameters.LEARNING_RATE) # Check https://pytorch.org/docs/stable/optim.html for other optimizers\n",
    "my_lr_scheduler = optim.lr_scheduler.StepLR(my_optimizer, step_size=25, gamma=0.1) # Check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate for other schedulers\n",
    "# my_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(my_optimizer,\n",
    "#                                                       factor=0.1, \n",
    "#                                                       patience=10,\n",
    "#                                                       threshold=0.000001\n",
    "#                                                       ) # Check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate for other schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ebbe8-74ed-4e6d-8e20-9aedbb05e3ff",
   "metadata": {},
   "source": [
    "## Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef98cd8-aef9-458f-9f02-e74fa7836f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/participant4/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([4, 3, 512, 512])\n",
      "########\n",
      "Epoch 0/4: TrainLoss: 0.4891, TrainAcc: 0.6659, ValLoss: 0.4983, ValAcc: 0.6948\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([9, 3, 512, 512])\n",
      "########\n",
      "########\n",
      "torch.Size([4, 3, 512, 512])\n",
      "########\n",
      "Epoch 1/4: TrainLoss: 0.4604, TrainAcc: 0.7532, ValLoss: 0.4549, ValAcc: 0.7777\n"
     ]
    }
   ],
   "source": [
    "train_main(model, class_count, ModelParameters.CRITERION, ModelParameters.EVAL_METRIC,\n",
    "    \n",
    "           # 1,                  # Epochs\n",
    "           ModelParameters.EPOCHS,\n",
    "           DEVICE, my_optimizer, \n",
    "           my_lr_scheduler, dataloaders, \n",
    "           logging = ModelParameters.LOGGING, \n",
    "           model_name = MODEL_FILENAME\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f22c9-31cf-45c4-b5e1-df21bda7012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cracknet_model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "cracknet_model_total_params = sum(p.numel() for p in cracknet_model.parameters())\n",
    "\n",
    "unet_model = UNet(num_classes = class_count)\n",
    "unet_model_total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "\n",
    "unet_resnet_model = UNetResnet(num_classes = class_count)\n",
    "unet_resnet_model_total_params = sum(p.numel() for p in unet_resnet_model.parameters())\n",
    "\n",
    "segnet_model = SegNet(num_classes = class_count)\n",
    "segnet_model_total_params  = sum(p.numel() for p in segnet_model.parameters())\n",
    "\n",
    "segresnet_model = SegResNet(num_classes = class_count)\n",
    "segresnet_model_total_params  = sum(p.numel() for p in segresnet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72315d-9840-40cd-a2d6-9bef818b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cracknet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe041f0-45c4-4322-a26d-041e2d5a5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f046dc-628b-4bcc-90ea-e27fe58c09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4db4e-1081-4135-a13b-d3ed44037a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaba51-8226-478c-95f9-001dad495d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "segresnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c4feb-3818-4d87-b54a-200ef2172c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
