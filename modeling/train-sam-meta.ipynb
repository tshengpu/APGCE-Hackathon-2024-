{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c77719-5d5b-4545-8e26-2b8f3512d8e7",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66952f5-e226-465e-abcc-e1eb59f48e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/participant4/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2024-11-17 04:02:16.270672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731816136.290666   88879 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731816136.296897   88879 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 04:02:16.317068: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import loss\n",
    "from torch import optim\n",
    "from metrics import eval_metrics, get_epoch_acc\n",
    "from dataloader import DataLoader\n",
    "from cross_val import CrossVal\n",
    "from torchvision import transforms\n",
    "from eval import eval\n",
    "from config import ModelParameters\n",
    "\n",
    "# Import available models, you can also explore other PyTorch models\n",
    "from cracknet import cracknet\n",
    "from unet import UNet, UNetResnet\n",
    "from segnet import SegNet, SegResNet\n",
    "\n",
    "# https://blog.roboflow.com/how-to-use-segment-anything-model-sam/\n",
    "from segment_anything import sam_model_registry\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7641c43b-aa23-4cd9-be9e-982df074a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757bf11-d774-4d60-a699-57a5e3bccdd6",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a19ce1-8fd3-4e9c-9432-04570d1b8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oneepoch(model, class_count, criterion, eval_metric, device, my_optimizer, my_lr_scheduler, dataloader):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    batch_acc_numerator = 0\n",
    "    batch_acc_denominator = 0\n",
    "    epoch_lr = my_lr_scheduler.get_last_lr()[0]\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        my_optimizer.zero_grad()\n",
    "        mask_pred = model(inputs)\n",
    "        loss = criterion(mask_pred, labels)\n",
    "        loss.backward()\n",
    "        my_optimizer.step()\n",
    "        \n",
    "        # batch_loss += loss\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch_acc_numerator_tmp, batch_acc_denominator_tmp = eval_metrics(mask_pred, labels, class_count, eval_metric)\n",
    "        batch_acc_numerator += batch_acc_numerator_tmp\n",
    "        batch_acc_denominator += batch_acc_denominator_tmp\n",
    "    my_lr_scheduler.step()\n",
    "    epoch_loss = batch_loss / len(dataloader)\n",
    "    epoch_acc = get_epoch_acc(batch_acc_numerator, batch_acc_denominator, eval_metric)\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca771314-be0b-459d-a212-1a389063161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model, class_count, criterion, eval_metric, EPOCHS, DEVICE, my_optimizer, my_lr_scheduler=None, dataloaders=None, logging=False, model_name='model.pt'):\n",
    "    model.to(DEVICE)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    lr = []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss, epoch_train_acc, epoch_lr = train_oneepoch(model, class_count, criterion, eval_metric, DEVICE, my_optimizer, my_lr_scheduler, dataloaders['train'])\n",
    "        epoch_val_loss, epoch_val_acc = eval(model, class_count, criterion, eval_metric, DEVICE, dataloaders['val'])\n",
    "\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if logging:\n",
    "            # train_loss.append(epoch_train_loss.detach().cpu().numpy().tolist())\n",
    "            train_loss.append(epoch_train_loss)\n",
    "            train_acc.append(epoch_train_acc)\n",
    "            # val_loss.append(epoch_val_loss.detach().cpu().numpy().tolist())\n",
    "            val_loss.append(epoch_val_loss)\n",
    "            val_acc.append(epoch_val_acc)\n",
    "            lr.append(epoch_lr)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS - 1}: TrainLoss: {epoch_train_loss:.4f}, TrainAcc: {epoch_train_acc:.4f}, ValLoss: {epoch_val_loss:.4f}, ValAcc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    print('Best Acc: {:4f}'.format(best_val_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    torch.save(model, model_name + '.pt')\n",
    "    \n",
    "    # save training details\n",
    "    pd.DataFrame({'Epochs':range(EPOCHS), 'Learning Rate': lr, 'Training Loss': train_loss, \n",
    "                    'Training Acc': train_acc, 'Validation Loss': val_loss, \n",
    "                    'Validation Acc': val_acc}).to_csv(model_name + '.csv', index = False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d559991-d6e0-4d87-8b61-41dab8da2f17",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3465706",
   "metadata": {},
   "source": [
    "Inside the data directory, the structure should be following:\n",
    "- train\n",
    "    - images\n",
    "        - IL 991.png\n",
    "        - IL 992.png\n",
    "    - labels\n",
    "        - IL 991.npy\n",
    "        - IL 992.npy\n",
    "    - class_names.txt\n",
    "- val\n",
    "    - images\n",
    "        - IL 993.png\n",
    "    - labels\n",
    "        - IL 993.npy\n",
    "    - class_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ab64e",
   "metadata": {},
   "source": [
    "class_names.txt is to specify the label class name for the training\n",
    "\n",
    "Example content of class_names.txt:\n",
    "\n",
    "\\_background_ <br>\n",
    "fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc9b1a1-ae9e-4422-8724-80f4c7d8a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the data directory and model filename\n",
    "DIR = 'data/' # Data directory\n",
    "MODEL_FILENAME = 'cracknet.pt' # Model filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfb8c8e-48da-494f-bc92-d490b109bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fault = []\n",
    "num_horizon = []\n",
    "num_seismic = []\n",
    "indeces = [v for v in range(1000, 1441)]\n",
    "for name in os.listdir('../data/raw_fault'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "\n",
    "    num_fault.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "for name in os.listdir('../data/raw_horizon'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    num_horizon.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "for name in os.listdir('../data/raw_seismic'):\n",
    "    if name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    num_seismic.append(int(name.split('-')[1].split('.')[0]))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'RAW_SEISMIC': [f\"seismic-{x}.png\" if x in num_seismic else None for x in indeces],\n",
    "    'RAW_FAULT': [f\"fault-{x}.npy\" if x in num_fault else None for x in indeces],\n",
    "    'RAW_HORIZON': [f\"horizon-{x}.npy\" if x in num_horizon else None for x in indeces]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9010f4-bc81-4cf7-b8d9-26200bd05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossVal(df, 3)\n",
    "dataloaders = cv[0]\n",
    "# dataset = {}\n",
    "# dataset['train'] = LabelMe(data_folder=os.path.join(DIR,'train'), transform=data_transforms['train'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataset['val'] = LabelMe(data_folder=os.path.join(DIR,'val'), transform=data_transforms['val'],\n",
    "#                                 img_size=(1024, 1024))\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size = BATCH_SIZE,\n",
    "#                                             shuffle = True, num_workers = 8, \n",
    "#                                             drop_last = False)\n",
    "#                                             for x in ['train', 'val']}\n",
    "class_count = len(dataloaders['train'].dataset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a2a6ec-bb90-470a-a0dc-c3c18015ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model for training, you can refer to the models that have been imported above\n",
    "# model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "# model = UNet(num_classes = class_count)\n",
    "\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "CHECKPOINT_PATH = \"meta_pretrained/sam_vit_h_4b8939.pth\"\n",
    "model = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
    "model.to(device=DEVICE)\n",
    "\n",
    "my_optimizer = optim.Adam(model.parameters(), lr = ModelParameters.LEARNING_RATE) # Check https://pytorch.org/docs/stable/optim.html for other optimizers\n",
    "my_lr_scheduler = optim.lr_scheduler.StepLR(my_optimizer, step_size=25, gamma=0.1) # Check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate for other schedulers\n",
    "# my_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(my_optimizer,\n",
    "#                                                       factor=0.1, \n",
    "#                                                       patience=10,\n",
    "#                                                       threshold=0.000001\n",
    "#                                                       ) # Check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate for other schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ebbe8-74ed-4e6d-8e20-9aedbb05e3ff",
   "metadata": {},
   "source": [
    "## Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef98cd8-aef9-458f-9f02-e74fa7836f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sam.forward() missing 1 required positional argument: 'multimask_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelParameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCRITERION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelParameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVAL_METRIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# 1,                  # Epochs\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mModelParameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmy_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mModelParameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOGGING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMODEL_FILENAME\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtrain_main\u001b[0;34m(model, class_count, criterion, eval_metric, EPOCHS, DEVICE, my_optimizer, my_lr_scheduler, dataloaders, logging, model_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 11\u001b[0m     epoch_train_loss, epoch_train_acc, epoch_lr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_oneepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     epoch_val_loss, epoch_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, class_count, criterion, eval_metric, DEVICE, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_val_acc \u001b[38;5;241m>\u001b[39m best_val_acc:\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mtrain_oneepoch\u001b[0;34m(model, class_count, criterion, eval_metric, device, my_optimizer, my_lr_scheduler, dataloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m my_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m mask_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(mask_pred, labels)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Sam.forward() missing 1 required positional argument: 'multimask_output'"
     ]
    }
   ],
   "source": [
    "train_main(model, class_count, ModelParameters.CRITERION, ModelParameters.EVAL_METRIC,\n",
    "           # 1,                  # Epochs\n",
    "           ModelParameters.EPOCHS,\n",
    "           DEVICE, my_optimizer, \n",
    "           my_lr_scheduler, dataloaders, \n",
    "           logging = ModelParameters.LOGGING, \n",
    "           model_name = MODEL_FILENAME\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f22c9-31cf-45c4-b5e1-df21bda7012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cracknet_model = cracknet(pretrained = ModelParameters.PRETRAINED, num_classes = class_count)\n",
    "cracknet_model_total_params = sum(p.numel() for p in cracknet_model.parameters())\n",
    "\n",
    "unet_model = UNet(num_classes = class_count)\n",
    "unet_model_total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "\n",
    "unet_resnet_model = UNetResnet(num_classes = class_count)\n",
    "unet_resnet_model_total_params = sum(p.numel() for p in unet_resnet_model.parameters())\n",
    "\n",
    "segnet_model = SegNet(num_classes = class_count)\n",
    "segnet_model_total_params  = sum(p.numel() for p in segnet_model.parameters())\n",
    "\n",
    "segresnet_model = SegResNet(num_classes = class_count)\n",
    "segresnet_model_total_params  = sum(p.numel() for p in segresnet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72315d-9840-40cd-a2d6-9bef818b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cracknet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe041f0-45c4-4322-a26d-041e2d5a5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f046dc-628b-4bcc-90ea-e27fe58c09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4db4e-1081-4135-a13b-d3ed44037a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "segnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaba51-8226-478c-95f9-001dad495d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "segresnet_model_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c4feb-3818-4d87-b54a-200ef2172c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
